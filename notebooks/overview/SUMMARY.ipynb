{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4efba8e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# GetYourMusicStyle Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bfdeb84",
   "metadata": {},
   "source": [
    "<img src=\"pictures/1.jpg\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0eb2b6",
   "metadata": {},
   "source": [
    "Being a musician and a music lover gave me some necessary domain knowledge to create this project:\n",
    "\n",
    "* You are at a live performance and would like to find music of  the same genre?\n",
    "* Your friend just recorded a cool demo and you want to find similar bands?\n",
    "* You are  trying to identify a recording they play in a coffee shop, but Shazam cannot find it,  \n",
    "and you are interested in discovering something  that sounds alike?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb6718",
   "metadata": {},
   "source": [
    "## Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca19f7d8",
   "metadata": {},
   "source": [
    "The goal of this effort is to create a tool that:\n",
    "* performs music genre classification\n",
    "* displays visualization of genre influences and how it is distributed during a music sample duration\n",
    "* recommends music of the same genre\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b953a4",
   "metadata": {},
   "source": [
    "1. ### DATA UNDERSTANDING and DATASET PREPARATION\n",
    "\n",
    "2. ### IDENTIFYING and EXTRACTING FEATURES\n",
    "\n",
    " - **features for regular NN as well as XGBoost**\n",
    " - **images for CNN  (spectragrams)**\n",
    "\n",
    "3. ### MODELS\n",
    "\n",
    "4. ### CLASSIFICAITON TOOL BASED ON MOST SUCCESFUL MODEL(S)\n",
    "\n",
    "5. ### PROOF OF CONCEPT DEMO RECOMMENDATION SYSTEM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed41d11a",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "## 1. DATA UNDERSTANDING and DASET PREPARATION\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f68081",
   "metadata": {},
   "source": [
    "* One of the significant challenges of this project was getting a good training dataset with proper genre labels. The standard option was the GTZAN music dataset (avaiable on kaggle.com), that is the most-used public dataset for evaluation in machine listening research for music genre recognition. But as mentioned in a [paper by Bob L. Sturm\n",
    "](https://arxiv.org/pdf/1306.1461v2.pdf) it has significant faults: genres are mislabeled, lots of repetetions, only 1000 samples based on less than 300 songs, noisy recordings, etc. After exploring the dataset, I decided to avoid using it and ended up creating my own dataset based on my own music collection as well as some publically available Youtube mixes/lists.  \n",
    "\n",
    "\n",
    "* Almost 2,000 recorded musical pieces of 14 genres were selected\n",
    "    \n",
    "    \n",
    "* Each raw music file was cut in 30 second chunks totalling to 4,700 sample files. On average 2-3 chunks per each of the 2000 music files are included in the dataset\n",
    "  \n",
    "  \n",
    "* The first 30 seconds of each file were skipped to exclude intros.\n",
    "  \n",
    "  \n",
    "* Only the most representative excerpts were selected to make sure that dataset consists of typical genres examples as much as possible  \n",
    "  \n",
    "  \n",
    "* The detailed code of the dataset creation is beyond the scope of this repository. In the future I plan to add more genres and selections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6b9cc",
   "metadata": {},
   "source": [
    "***  \n",
    "## 2. IDENTIFYING AND EXTRACTING FEATURES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c44f68",
   "metadata": {},
   "source": [
    "***\n",
    "* A detailed feature exploration and visualizations are included in 'EDA and Features Exploring.ipynb'  \n",
    "\n",
    "\n",
    "* src/generate_features.py script (takes several hours to execute) was used to generate features and spectrograms (see the details below)  \n",
    "\n",
    "\n",
    "* Here are some key points for feature extractions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b9bb3e",
   "metadata": {},
   "source": [
    "### Two Approaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ca45e",
   "metadata": {},
   "source": [
    "After an initial load of an audio file:  **x , sr = librosa.load(audio_file)** :\n",
    "\n",
    "1. **Numeric Features extraction:** Using various transformations over **x** extract chosen features and store them in a CSV file together with the genre labels. This CSV file will be used in model training (regular NN or models like XGBoost)  \n",
    "\n",
    "\n",
    "2. **Spectrograms generation:** Instead of extracting numeric features, we can generate a spectrogram image of a sound. Such spectrogram visualizes the signal strength over time at various frequencies. A model can be trained directly using images/spectrograms (good for CNN network training) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a7ca8",
   "metadata": {},
   "source": [
    "#### 1. Numeric Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728d1956",
   "metadata": {},
   "source": [
    "\n",
    "* librosa library was used for feature extraction\n",
    "\n",
    "\n",
    "* Initially 7 characteristics were chosen: rmse, chroma_shft, central_centroid, central_rolloff, central_bandwidth, Zero Crossing Rate, and 20 mfcc channels. Also cqt and tonnets were considered.  \n",
    "\n",
    "\n",
    "* After several iterations of testing the following was chosen:  \n",
    "* * 13 mfcc channels, 7 spectral_contrasts channels,  \n",
    "* * the  means of: spectral_bandwidth, zero_crossing_rate, chroma_shft, and rmse for both harmonic and percussive wave split.  \n",
    "\n",
    "Below is a full feature list and their importance report (as generated by XGBoost model at a later modeling stage):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e56af",
   "metadata": {},
   "source": [
    "<img src=\"EDA/feature_importance.png\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f229907",
   "metadata": {},
   "source": [
    "#### 2. Spectrogram Images generation:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcbbae14",
   "metadata": {},
   "source": [
    "<img src=\"EDA/spectrograms1.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e795f",
   "metadata": {},
   "source": [
    "For CNN model after trying different spcectrograms visualizations  \n",
    "log transformations were performed in librosa to make it more readable for CNN model:  \n",
    "* librosa.display.specshow(ampl, x_axis='time',  y_axis='log')  \n",
    "\n",
    "<img src=\"EDA/log_spectrograms2.png\" alt=\"drawing\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749946a6",
   "metadata": {},
   "source": [
    "***\n",
    "# 3. MODELING\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d02d125",
   "metadata": {},
   "source": [
    "* Three notebooks  (XGB.ipynb, NN.ipynb, CNN.ipynb) have all the training/evaluation details\n",
    "\n",
    "Quick Overview of the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a691bd",
   "metadata": {},
   "source": [
    "## XGB and NN models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f8fa13",
   "metadata": {},
   "source": [
    "* The features/features.csv file - is used to load features/target for training/evaluating  \n",
    "\n",
    "\n",
    "* OPTIONALLY: features.csv can be regenerated by running src/generate_features.py (uncomment: extract = 'FEATURES' at the top of the script)  might takes several hours to complete the process  \n",
    "\n",
    "\n",
    "* Accuracy of 96% (for NN model) and 92% (for XGBooster model) is achieved  (please check corresponding notebooks for the details)\n",
    "\n",
    "Here is training process for NN: \n",
    "\n",
    "<img src=\"EDA/learning curve NN.JPG\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "**Further details about modeling are in corresponding XGB.ipynb and NN.ipynb notebooks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21700e60",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a56111b",
   "metadata": {},
   "source": [
    "* GPU enironment in Google Collab was used for training  \n",
    "\n",
    "* OPTIONALLY: all the images can be regenerated by running src/generate_features.py (uncomment: extract = 'IMAGES' at the top of the script)  \n",
    "\n",
    "* (100 x 100, 23,000+ images,  6 seconds span  \n",
    "\n",
    "* Best models are saved in models folder\n",
    "\n",
    "* Accuracy of 94% (for CNN) was achieved. Further improvements are possible if adding more samples.\n",
    "\n",
    "\n",
    "<img src=\"EDA/CNN Training.PNG\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db30f78",
   "metadata": {},
   "source": [
    "4. ### CLASSIFICAITON TOOL BASED ON MOST SUCCESFUL MODEL(S)\n",
    "\n",
    "5. ### PROOF OF CONCEPT DEMO RECOMMENDATION SYSTEM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
